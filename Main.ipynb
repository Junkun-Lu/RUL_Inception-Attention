{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experiment.Experiment import Exp_Inception_Attention\n",
    "from Model.CNN_Layer_CMAPSS import CNN_Layers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "    \n",
    "args = dotdict()\n",
    "\n",
    "\"\"\"\n",
    "dataset_name: choose the dataset we used now\n",
    "\n",
    "sequence_length: euqal to max_len\n",
    "MAXLIFE: the knee point of RUL in each engine's RUL --- for CMAPSS\n",
    "\n",
    "n_heads_full: the head num of FullAttention, used for ablation study\n",
    "n_heads_log: the head num of LogSparseAttention\n",
    "n_heads_local: the head num of LocalAttention\n",
    "n_heads_prob: the head num of ProbSparseAttention\n",
    "n_heads_FFT: the head num of FFT\n",
    "n_heads_auto: the head num of AutoCorrelation\n",
    "\n",
    "moving_avg: the kernel_size for decomposition block\n",
    "n_layers: the number of layers in Encoder\n",
    "predictor_type: choose which Predictor used\n",
    "\n",
    "learning_rate_patience: learning rate change after learning_rate_patience's epoch\n",
    "learning_rate_factor: the percentage of learning rate change\n",
    "arly_stop_patience: the time of training stop, when vali loss always bigger\n",
    "\n",
    "enc_pred_loss: the loss function we choosed\n",
    "sigma_faktor: The larger the value, the smaller the simga and the narrower the distribution of weights\n",
    "anteil: The larger the value, the larger the subsequent distribution\n",
    "smooth_loss: use the smooth_loss or not\n",
    "\"\"\"\n",
    "\n",
    "# dataload_parameter\n",
    "args.dataset_name     = \"CMAPSS\"   # CMAPSS/FEMTO/XJTU\n",
    "\n",
    "args.data_path        = \"D:/study/Master-Thesis/数据集-RUL/CMaps/\"\n",
    "args.Data_id          = \"FD001\"\n",
    "args.sequence_length  = 36\n",
    "args.MAXLIFE          = 120\n",
    "args.difference       = False\n",
    "args.normalization    = \"minmax\"\n",
    "args.validation       = 0.1\n",
    "args.batch_size       = 32\n",
    "\n",
    "# model_parameter\n",
    "args.dropout = 0.2\n",
    "args.max_len = 36\n",
    "args.d_model = 17\n",
    "\n",
    "args.n_heads_full  = 0\n",
    "args.n_heads_log   = 1\n",
    "args.n_heads_local = 1\n",
    "args.n_heads_prob  = 1\n",
    "args.n_heads_FFT   = 1\n",
    "args.n_heads_auto  = 1\n",
    "\n",
    "args.d_k        = 48\n",
    "args.d_v        = 48\n",
    "args.moving_avg = 25\n",
    "args.d_ff       = 48\n",
    "args.n_layers   = 1\n",
    "args.device     = torch.device('cuda:{}'.format(0))\n",
    "\n",
    "args.predictor_type = \"hybrid\"\n",
    "\n",
    "# training_parameter\n",
    "args.train_epochs           = 200\n",
    "args.optimizer              = \"Adam\"\n",
    "args.learning_rate          = 0.001\n",
    "args.learning_rate_patience = 10\n",
    "args.learning_rate_factor   = 0.3\n",
    "args.early_stop_patience    = 45\n",
    "\n",
    "# loss\n",
    "args.sigma_faktor         = 15   # 越大 simga越小  权重分布就越窄\n",
    "args.anteil               = 40   # 越大 比重反差就越大，后面的分布也就越大\n",
    "args.enc_pred_loss        = \"WeightMSE\"  # \"MSE\"\n",
    "args.weight_type          = \"gaussian\"\n",
    "args.smooth_loss          = \"smooth_mse\"  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Exp_Inception_Attention(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"FD001\"\n",
    "exp.train(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
